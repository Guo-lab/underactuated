<!DOCTYPE html>

<html>

  <head>
    <title>Ch. 21 - Imitation Learning</title>
    <meta name="Ch. 21 - Imitation Learning" content="text/html; charset=utf-8;" />
    <link rel="canonical" href="http://underactuated.mit.edu/imitation.html" />

    <script src="https://hypothes.is/embed.js" async></script>
    <script type="text/javascript" src="chapters.js"></script>
    <script type="text/javascript" src="htmlbook/book.js"></script>

    <script src="htmlbook/mathjax-config.js" defer></script>
    <script type="text/javascript" id="MathJax-script" defer
      src="htmlbook/MathJax/es5/tex-chtml.js">
    </script>
    <script>window.MathJax || document.write('<script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" defer><\/script>')</script>

    <link rel="stylesheet" href="htmlbook/highlight/styles/default.css">
    <script src="htmlbook/highlight/highlight.pack.js"></script> <!-- http://highlightjs.readthedocs.io/en/latest/css-classes-reference.html#language-names-and-aliases -->
    <script>hljs.initHighlightingOnLoad();</script>

    <link rel="stylesheet" type="text/css" href="htmlbook/book.css" />
  </head>

<body onload="loadChapter('underactuated');">

<div data-type="titlepage">
  <header>
    <h1><a href="index.html" style="text-decoration:none;">Underactuated Robotics</a></h1>
    <p data-type="subtitle">Algorithms for Walking, Running, Swimming, Flying, and Manipulation</p>
    <p style="font-size: 18px;"><a href="http://people.csail.mit.edu/russt/">Russ Tedrake</a></p>
    <p style="font-size: 14px; text-align: right;">
      &copy; Russ Tedrake, 2024<br/>
      Last modified <span id="last_modified"></span>.</br>
      <script>
      var d = new Date(document.lastModified);
      document.getElementById("last_modified").innerHTML = d.getFullYear() + "-" + (d.getMonth()+1) + "-" + d.getDate();</script>
      <a href="misc.html">How to cite these notes, use annotations, and give feedback.</a><br/>
    </p>
  </header>
</div>

<p><b>Note:</b> These are working notes used for <a
href="https://underactuated.csail.mit.edu/Spring2024/">a course being taught
at MIT</a>. They will be updated throughout the Spring 2024 semester.  <a
href="https://www.youtube.com/playlist?list=PLkx8KyIQkMfU5szP43GlE_S1QGSPQfL9s">Lecture videos are available on YouTube</a>.</p>

<table style="width:100%;"><tr style="width:100%">
  <td style="width:33%;text-align:left;"><a class="previous_chapter" href=rl_policy_search.html>Previous Chapter</a></td>
  <td style="width:33%;text-align:center;"><a href=index.html>Table of contents</a></td>
  <td style="width:33%;text-align:right;"><a class="next_chapter" href=drake.html>Next Chapter</a></td>
</tr></table>

<script type="text/javascript">document.write(notebook_header('imitation'))
</script>
<!-- EVERYTHING ABOVE THIS LINE IS OVERWRITTEN BY THE INSTALL SCRIPT -->
<chapter style="counter-reset: chapter 20"><h1>Imitation Learning</h1>

  <p>Imitation learning, also known as "learning from demonstrations" (LfD), is the
  problem of learning a policy from a collection of demonstrations. For state-based
  feedback, these demonstrations take the form of a set of state-action sequences,
  $\left[ \bx[\cdot], \bu[\cdot]\right]$. For the richer class of <a
  href="output_feedback.html">output feedback</a>, this takes the form of
  observation-action sequences, $\left[ \by[\cdot], \bu[\cdot]\right]$. Note that we do
  not require any explicit definition of a cost or reward function; in most cases we
  assume that the demonstrations are obtained from an optimal or near-optimal policy.
  </p>
  
  <p>Broadly speaking, most approaches to imitation learning can be categorized as
  either <i>behavior cloning</i> (BC) or <i>inverse reinforcement learning</i> (IRL).
  Behavior cloning attempts to learn a policy directly from the data using supervised
  learning. Inverse RL (aka inverse optimal control) attempts to learn a cost function
  from the data, and then uses potentially more traditional optimal control approaches
  to synthesize a policy for this cost function, in the hopes of generalizing
  significantly beyond the demonstration data. See
  <elib>Argall09+Bagnell15+Osa18</elib> for some fairly recent surveys, though
  admittedly they appeared a bit before the latest boom.</p>
  
  <p>I think it's fair to say that today, in 2024, behavior cloning is once again taking
  the robotics world by storm (especially in manipulation research), and it now seems
  like the shortest path to building "generalist" robot foundation-model-style policies.
  We'll devote most of this chapter to it.</p>

  <section><h1>Behavior cloning</h1>

    <p>Behavior cloning <elib>Pomerleau88+Bain95</elib> attempts to learn a policy
    directly from the policy's input-output data using supervised learning. In the case
    of full-state feedback, where we know that the optimal policy can be represented as
    a simple function of the state, this is typically cast as a simple regression
    problem, e.g. $\min_\theta \sum_{i} |\bu_i - \pi_\theta(\bx_i)|^2,$ where $\theta$
    are the parameters of e.g. the neural network. In the richer case of <a
    href="output_feedback.html">output feedback</a>, then this becomes a sequence
    learning problem (we are learning a dynamical system which transforms a sequence of
    observations into a sequence of actions).</p>
    
    <p>Famously, Large Language Models (LLMs) are trained with behavior cloning (and
    then fine-tuned to make them more aligned with human preferences
    <elib>Ziegler19+Zhang24</elib>). OpenAI's GPT models are <a
    href="sysid.html#autoregressive">autoregressive models</a> that predict the next
    token given a "context" of recent tokens
    <elib>Radford18+Radford19+Brown20+Bubeck23</elib>. More recently, we've seen the
    extension to multi-modal models, such as visually-conditioned language models (VLMs)
    like Flamingo, GPT-4V, and LLaVa <elib>Liu23</elib> with an increasing wealth of
    open-source reproductions <elib>Karamcheti24+Laurençon24</elib>.</p>
    
    <p>Is predicting actions fundamentally different from next-token prediction in
    language? There are a few reasons why it might be. Actions are continuous and
    high-dimensional, whereas language tokens are discrete. Our control systems get put
    into the feedback loop with physics, and have to deal with stochasticity from the
    environment that LLMs don't experience. Google DeepMind released the RT series of
    "vision-language-action" (VLA) models (RT-1 <elib>Brohan22</elib>, RT-2 <elib>Brohan23</elib>, and RT-X <elib>O’Neill24</elib>) which started to show
    that predicting actions may not be so fundamentally different than language, at
    least for simple pick-and-place tasks. Then in 2023, there were two main lines of
    work which convincingly demonstrated this capability could even extend to
    surprisngly dexterous manipulation. Those were the <a
    href="https://diffusion-policy.cs.columbia.edu/">Diffusion Policy</a>
    <elib>Chi24</elib> and the Action Chunking Transformer (ACT) from the <a
    href="https://tonyzhaozh.github.io/aloha/">ALOHA project</a>
    <elib>Zhao23</elib>. Of course, these were built on a long line of progress on
    "visuomotor policies" which we'll discuss below.</p>

    <figure>
    <iframe width="560" height="315" src="https://www.youtube.com/embed/w-CGSQAO5-Q?si=tNIJzVegfsIwb7tK" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
    <figcaption>Diffusion Policy started as an intern project(!) by Cheng Chi at TRI,
    and blossomed into a great collaboration with Shuran Song.</figcaption>
    </figure>

    <p>Diffusion Policy and ALOHA seem to have been the watershed results for dexterous
    manipulation in robotics. Since that time, the internet is now rich with videos of
    highly dexterous manipulation from all sorts of robots ranging from very low cost
    manipulators up to and including humanoid robots with dexterous hands.</p>

    <p>In one sense, it might seem a little disappointing that, after we've spent so
    much time in these notes exploring the rich mathematical foundations of dynamics and
    control, that behavior cloning from human teleop demonstrations can outperform out
    best methods for some class of problems (which are arguably more about understanding
    the world than about dynamics and control). But think of it this way: using
    supervised learning is an awfully clever way to explore the space of policy
    parameterizations, and has accelerated us into bigger questions about using cameras
    in the feedback loop, learning multitask/foundation models, and leveraging structure
    (such as 3D geometry / objectness) in our representations or not. The success of
    LLMs (and now multimodal models) is undeniable, and it would be a mistake to ignore
    the great new possibilities that have opened up. And I do think that there is
    something fundamentally about imitation learning for manipulation: in many cases the
    "rules of the game" (e.g. task specification) is not fully described by physics --
    I've come to appreciate that humans bring an amazing amount of background knowledge
    / common sense to bear when they are performing even relatively simple manipulation
    tasks. I'm very confident that our knowledge of dynamics and control will help us
    penetrate the new vistas enabled by these high-capacity models.</p>
    
    <p>Let me make one more important point. Sometimes people say that BC is
    fundamentally limited because it can never outperform the human demonstrator. But
    one of the early results from behavior cloning was that this limit that is not
    strictly true. <todo>cite the classic result; i forget which classic paper it came from. Chowdery23
    figure 6 is relevant, but they seem to hedge a bit because perhaps the metric isn't
    great.</todo> Certainly Chat-GPT can produce text output that by some metrics
    surpasses the capabilities of any one human. <a
    href="https://kempnerinstitute.harvard.edu/research/deeper-learning/transcendence-generative-models-can-outperform-the-experts-that-train-them/">Here</a>
    is a recent case study of this phenomenon for chess.</p>
  
    <p>There is also a precident for combining BC with other methods to improve beyond
    the original demonstrations. For DeepMind's AlphaGo<elib>Silver16</elib>, behavior
    cloning was only the first step -- it was enough to get us to a strong Go player,
    but not enough to be championship level. Adding "self-play" and improvement via
    Monte-Carlo tree search was fundamental to "mastering the game of Go". </p>

    <subsection id="visuomotor"><h1>Visuomotor policies (aka control from pixels)</h1>
    
      <p>In 2016, a few years after the start of the deep learning revolution,
      <elib>Levine16</elib> introduced the concept of a "visuomotor policy", and
      produced <a href="https://www.youtube.com/watch?v=Q4bMcUk6pcw">fairly stunning
      videos</a> of robots performing complicated tasks. The name "visuomotor" is chosen
      to emphasize that the policy is trained to predict actions directly from RGB
      camera images, using architectures similar to the one pictured below; in a sense
      this was a return to the early ideas of <elib>Pomerleau88</elib>, but now powered
      with deep learning architectures and pretraining.</p>
      
      <figure><img width="60%" src="figures/visuomotor.svg"/><figcaption>The (now)
      classical visuomotor policy architecture, adapted from
      <elib>Levine16+Florence20</elib>. The emphasis here is that $z$ is a <i>learned
      state representation</i> which encodes the state of the environment.
      </figcaption></figure>

      <p>My own journey with imitation learning started with a <a
      href="https://sites.google.com/view/visuomotor-correspondence">project led by Pete
      Florence and Lucas Manuelli</a> on using a particular form of self-supervised
      learning for 3D geometries to train the policies <elib>Florence20</elib>. For me,
      this was the first time I got to really experience directly how the model was
      trained, and I got to experience the incredibly impressive robustness that was
      possible in the roll-outs, even with a relatively modest amount of robot training
      data.</p>

      <p>Writing feedback controllers which operate directly on the RGB camera images
      was something entirely new for me. We had been using RGB-D cameras to do some
      amount of visual state estimation / pose estimation before that, but were leaning
      pretty heavily on the depth channel and very explicit 3D reasoning/matching. But I
      remember around the time of our first imitation learning project, I asked the
      students "if you could only choose one, RGB <i>or</i> depth, which would you
      choose". They chose RGB. There are many situations where a task is unclear or even
      ambiguous when looking only at a depth image. The ambiguities can often be
      resolved with the addition of RGB, and there are many depth cues in RGB that allow
      us and our visuomotor policies to be successfull for 3D tasks even without an
      explicit depth sensor. Diffusion Policy and ACT, for instance, achieve their
      amazing performance by consuming RGB (no depth).</p>

      <p>In my experience, control theorists have very satisfying answers to almost any
      dynamics and control problem. But (with a few rare exceptions) they didn't do
      computer vision. This was something entirely new. The sensor model -- the mapping
      from, e.g. the state and parameters of a <code>MultibodyPlant</code> to the output
      image $y$ -- is potentially a full game-engine-quality renderer. Even though there
      are lots of projects now on making differentiable renderers, these can only do so
      much because the pixelation process is inherently very local/non-smooth. Going
      from the image back into a manageable intermediate (latent) state representation,
      $z$, started becoming viable with the rise of deep networks for perception.
      Data/learning <i>does</i> feel fundamental here -- mapping from RGB into a
      meaningful representation for control is more about the statistics of natural
      scenes than about the model-based physics of propagating light.</p>

      <p>This, for me, was the first main lesson I took from our imitation learning
      work: closing feedback loops by directly consuming RGB at control rates is now
      possible, and is incredibly powerful for robust performance in visually complex
      settings. Imitation learning (and reinforcement learning) have so far enabled this
      in a way that model-based control pipelines which require explicit state (or
      belief) estimation do not. (Of course, techniques like teacher-student
      distillation <elib>Chen20+Hwangbo19</elib> and <elib>Chou23</elib> might help us
      bridge that gap.)</p>

    </subsection>

    <subsection><h1>Behavior cloning as sequence modeling</h1>
    
     <!-- note: already mentioned this once above. -->
     <!-- aka system id. as we know, for output feedback, even the optimal policy might
require a history of observations. -->
     <!-- bagnell has some classic papers on this -->

    </subsection>
  
    <subsection><h1>Supervised learning in a feedback loop: dealing with distribution
    shift</h1>
    
      <p>One of the famous challenges in imitation learning is the problem of
      distribution shift. Imagine that you are training a policy for driving a car...
      </p>

      <p>DAGGER <elib>Ross11</elib></p>
    
      <p>Teacher/Student.</p>

    </subsection>

    <subsection><h1>Dealing with suboptimal and multimodal demonstrations</h1>

      <todo>Discrete maze example w/ symmetries. Push-T example.</todo>

      <todo>is multimodality at odds with (local) stability? cite max's neurips
      paper.</todo>
    </subsection>

  </section>

  <section><h1>Architectures for visuomotor policies</h1>
  
    <subsection><h1>Desiderata</h1>

      <p>scalable, reliable training, ..., can cope with multimodality, ...</p>

    </subsection>

    <subsection><h1>Output/action decoders</h1>

      <p>The visuomotor policies that we'll study here should output low-level robot
      actions -- $\bu$ in the parlance of these notes. These need not be torque commands
      directly... in fact it's more typical for them to output a slightly higher-level
      command like joint velocity or end-effector velocity, which gets passed to a
      low-level controller. Note that there is also a now large body of literature where
      people use LLMs or VLMs to determine a sequence of high level actions, but assume
      that someone has authored or otherwise obtained a set of "skill libraries" that
      map the discrete high-level actions to control; while interesting, I would not
      call those approached visuomotor policies and will not discuss those here.</p>
      
      <p>In all cases, the input encoders (discussed next) map the recent history of
      observations into some latent representation, which then eventually gets mapped
      back into actions via the action decoder. It is quite useful to categorize the
      different visuomotor policy architectures based on the different choices that they
      make about the action decoder.
      </p>

      <p>There is a series of work, now commonly referred to as VLA
      (vision-language-action) architectures, which leverage the successful transformer architectures from language and vision by discretizing and tokenize the robot
      action space. Early examples include Decision Transformer <elib>Chen21</elib>,
      Gato<elib>Reed22</elib>, and more recently these powered the Robot Transformer
      (RT) line of models from Google DeepMind: RT-1 <elib>Brohan22</elib>, RT-2
      <elib>Brohan23</elib>, RT-X
      <elib>O’Neill24</elib>. For example, in RT, the action is represented using a
      uniform discretization with 256 bins over each coordinate (using the 1st and 99th
      quantile of the actions in the training data as the minumum and maximum values,
      respectively); in those models the action space was taken to be the end-effector
      pose and gripper extension. Then they literally output a string of integers
      corresponding to those coordinates as desired text output for the VLM; if the
      integers not already represented as tokens in the VLM, then they simply overwrite
      the 256 least frequently used tokens
      <elib>Brohan23</elib>. You can find an strong open-source reproduction of RT-2X
      models in the <a href="https://openvla.github.io/">OpenVLA project</a>
      <elib>Kim24</elib>.</p>
      
      <!-- maybe use the graph search formulation from decision transformer as an example here? it's nice and conceptually simple. -->
  
      <p>These tokenized-action architectures naturally learn a probability over next
      tokens, which deals very directly with the potential multimodality in the training
      data. But this comes at the cost of having discretized the action space. I don't
      worry much about the resolution of this discretization being limiting, but I'm a
      bit worried that the discretization destroys the natural inductive bias of the
      continuous space. (For instance, the end-effector position 0.1 is closer to 0.2
      than to 0.5, but this information is completely discarded in the discretization.)
      Other people don't seem as concerned, and perhaps when we eventually have enough
      data it will all be in the noise.</p>

      <p>Behavior Transformers (BeT) <elib>Shafiullah22</elib> and VQ-BeT
      <elib>Lee24</elib> put more work into discretizing the action space, by
      preprocessing the data with k-means (in BeT) or Residual Vector Quantization (in
      VQ-BeT), and then doing some continuous up-sampling.</p> 
  
      <p>There were a number of attempts to handle multimodality in a more natively
      continous setting. Implicit BC <elib>Florence22</elib> called this out very
      explicitly, and also explored the use of Mixture Density Networks
      <elib>Bishop94</elib> for behavior cloning. But things really started working well
      when some of the new tools from generative AI started getting applied to robotics;
      most notably in the Action-chunking transformers (ACT) <elib>Zhao23</elib> which
      used Conditional <a
      href="https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73">Variational
      Autoencoders</a> (CVAE) and in Diffusion Policy <elib>Chi24</elib> which used
      Diffusion models. We'll discuss these in more detail below.</p>

      <p>Both the ACT paper and the Diffusion Policy paper strongly emphasized another
      detail about the output encoding: rather than predicting a single (current) action
      to take, these models predicted an entire sequence of future actions, and then
      operate in a fashion similar to model-predictive control.</p>

      <!-- PerceiverActor is another candidate for this section; it uses voxelized 6-dof
poses for the action space. But it's perhaps not quite as general as the others, so I'm
not sure if I want to include it here. -->

    </subsection>

    <subsection><h1>(Multi-modal) input encoders</h1>

      <p>Although researchers are rapidly adopting additional input modalities, by far
      the most common input modalities are the robot proprioception (e.g. joint
      sensors), which can be passed into the model directly, and image observations
      which need to be encoded from raw RGB into some intermediate representation.
      Although there is a torrent of literature on this, there are a few choices that
      have clearly emerged as the standards: ResNet and ViT. For instance, the original
      Diffusion Policy paper used a ResNet-18 (without pretraining) with small
      modifications, e.g. to maintain spatial information <elib>Chi24</elib>, but as the
      scale of the experiments has increased we have seen more success with
      CLIP-pretrained ViT <elib>Chi24a</elib>. 

      <!-- there is also considerable work on making equivariant, etc. platt, bohg, ..-->

      <p>Language-conditioned multitask policies... <elib>O’Neill24+Kim24</elib>.</p>

      <!--  Palm-E <elib>Driess23</elib>, Prismatic, Pi0 ... -->
    </subsection>

  </section>

  <section><h1>Diffusion Policy</h1>
  
    <p>One particularly successful form of behavior cloning for visuomotor
    policies with continuous action spaces is the <a
    href="https://diffusion-policy.cs.columbia.edu/">Diffusion Policy</a>
    <elib>Chi24</elib>. The dexterous manipulation team at TRI had been working
    on behavior cloning for some time, but the Diffusion Policy (which started
    as a summer internship project!) architecture has allowed us to very
    reliably train <a
    href="https://www.youtube.com/watch?v=w-CGSQAO5-Q">incredibly dexterous
    tasks</a> and really start to scale up our ambitions for manipulation.</p>

    <subsection><h1>Denoising Diffusion models</h1>
    
      <p>"Denoising Diffusion" models are an approach to generative AI, made famous by
      their ability to generate high-resolution photorealistic images. Inspired by the
      "manifold hypothesis" (e.g. the idea that realistic images live on a
      low-dimensional manifold in pixel space), the intuition behind denoising diffusion
      is that we train a model by adding noise to samples drawn from the data
      distribution, then learn to predict the noise from the noisy images, in order to
      "denoise" random images back on to the manifold. While image generation made these
      models famous, they have proven to be highly capable in generating samples from a
      wide variety of high-dimension continuous distributions, even distributions that
      are conditioned on high-dimensional inputs. I recommend <a
      href="https://chenyang.co/diffusion.html">this blog post</a> and
      <elib>Nakkiran24</elib> as excellent introductions.</p>
      
      <p>Let's consider samples $\bu \in \Re^m$ drawn from a training dataset
      $\mathcal{D}.$ Diffusion models are trained to estimate a noise vector
      ${\bf \epsilon} \in \Re^m$ to minimize the loss function $$\ell(\theta) =
      \mathbb{E}_{\bu, {\bf \epsilon}, \sigma} || {\bf f}_\theta(\bu + \sigma
      {\bf \epsilon}, \sigma ) - {\bf \epsilon} ||^2,$$ where $\theta$ is the
      parameter vector, and $f_\theta$ is typically some high-capacity neural
      network. In practice, training is done by randomly sampling $\bu$ from
      $\mathcal{D}$, ${\bf \epsilon}$ from $\mathcal{N}({\bf 0}_m, {\bf I}_{m
      \times m})$, and $\sigma$ from a uniform distribution over a positive set
      of numbers denoted as $\{\sigma_k\}_{k=0}^K,$ where we have $\sigma_k >
      \sigma_{k-1}.$</p>

      <p>To sample a new output from the model, the denoising diffusion
      implicit models (DDIM) sampler <elib>Song20</elib> takes multiple steps:
      $$\bu_{k-1} = \bu_k + (\sigma_{k-1} - \sigma_k)f_\theta(\bu_k,
      \sigma_k).$$ This specific parameterization of the update (and my
      preferred notation more generally) comes from
      <elib>Permenter23</elib>. I've presented the deterministic denoiser here, but some
      analysis suggests that Langevin sampling can improve performance if one takes many
      denoising steps<elib>Xu23</elib>.</p>
      
      <p>Diffusion models have a slightly convoluted history. The term
      "diffusion" came from a paper <elib>Sohl-Dickstein15</elib>
      which used an analogy from thermodynamics to use a prescribed diffusion
      process to slowly transform data into random noise, and then learned to
      reverse this procedure by training an inverse diffusion. Well before
      that, a series of work starting with <elib>Hyvarinen05</elib> studied the
      problem of learning the score function (the gradient of the log
      probability of a distribution) of a data distribution, and
      <elib>Vincent11</elib> made a connection to denoising autoencoders. 
      <elib>Song19</elib> put all of this together beautifully and combined it
      with deep learning to propose denoising diffusion as a generative
      modeling techinque. They learned a single network that was conditioned on
      the noise level. This was followed quickly by <elib>Ho20</elib>
      which introduced denoise diffusion probabilistic models (DDPM) using an
      even simpler update and showed results competitive with other leading
      generative modeling techniques leading to <elib>Song20</elib> giving us
      the DDIM update above. <elib>Permenter23</elib> gives a deterministic
      interpretation as learning the distance function from the data manifold,
      and sampling as performing approximate gradient descent on this
      function.</p>
        
      <todo>Examples! Something like Figure 1 from Sohl-Dickstein15 would be
      good.</todo>

      <p>It is straight-forward to condition the generative model on an
      exogeneous input, by simply adding an additional signal, $\by$, to the
      denoiser: $f_\theta(\bu, \sigma, \by).$ </p>

    </subsection>

    <subsection id="diffusion_policy"><h1>Diffusion Policy</h1>

      <p>Behavior cloning is perhaps the simplest form of imitation learning --
      it simply attempts learn a policy using supervised learning to match
      expert demonstrations. While it is tempting to learn deterministic
      output-feedback policies (maps from history of observations to actions),
      one quickly finds that human demonstrations are typically not unique.
      Perhaps this is not surprising, as we know that optimal feedback policies
      in general are not unique! To address this non-uniqueness /
      multi-modality in the human demonstrations, it's well understood that
      behavior cloning benefits from learning a conditional
      <i>distribution</i> over actions.</p>
      
      <p>Diffusion Policy is the natural application of (conditional) denoising
      diffusion models to learning these policies. It was inspired, in
      particular, but the modeling choices in Diffuser<elib>Janner22</elib>. In
      particular, rather than generating a single action, the denoiser in
      diffusion policy outputs a sequence of actions with horizon $H_u$; like
      <elib>Zhao23</elib> we found experimentally that this leads to more
      stable roll-outs. <elib>Block23b</elib> provides some possible
      theoretical justification for this choice. We condition the input on a
      history of observations of length $H_y.$</p>

    </subsection>

    <subsection id="dp_lqg"><h1>Diffusion Policy for Linear Policies</h1>
    
      <p>Let me be clear, it almost certainly does <i>not</i> make sense to use a
      diffusion policy to represent a linear (output) feedback control policy. But it
      can be helpful to understand what the Diffusion Policy looks like in this
      extremely simplified case.</p>

      <p>Consider the case where we have the standard linear-Gaussian dynamical
      system: \begin{gather*} \bx[n+1] = \bA\bx[n] + \bB\bu[n] + \bw[n], \\
      \by[n] = \bC\bx[n] + \bD\bu[n] + \bv[n], \\ \bw[n] \sim \mathcal{N}({\bf
      0}, {\bf \Sigma}_w), \quad \bv[n] \sim \mathcal{N}({\bf 0}, {\bf
      \Sigma}_v). \end{gather*} Imagine that we create a dataset by rolling out
      trajectory demonstrations using a linear (output) feedback policy -- it could be, for instance, the optimal policy from LQG design. The question is:
      what (exactly) does the diffusion policy learn?</p>
    
      <subsubsection><h1>State-feedback</h1>

      <p>Let's start with the state-feedback case, where we generate roll-outs using a
      controller, $\bu = - \bK\bx$, given a Gaussian distribution of intial conditions
      and Gaussian process noise. In this case, the training loss function reduces to
      $$\ell(\theta) = \mathbb{E}_{\bx, {\bf \epsilon}, \sigma} || {\bf
      f}_\theta(-\bK\bx + \sigma {\bf \epsilon}, \sigma, \bx) - {\bf \epsilon} ||^2,$$
      where the expectation in $\bx$ is over the <a
      href="policy_search.html#lqr">stationary distribution of the closed-loop
      system</a>. In this case, we don't need a neural network; take $f_\theta$ to be a
      simple function. In particular, we can achieve zero loss using the denoiser given
      by $${\bf f}_\theta(\bu, \sigma, \bx) = \frac{1}{\sigma}\left[\bu +
      \bK\bx\right].$$ At evaluation time, the sampling iterations, $$\bu_{k-1} = \bu_k
      + \frac{\sigma_{k-1} - \sigma_k}{\sigma_k}\left[\bu_k + \bK\bx\right],$$ will
      converge on $\bu_0 = -\bK\bx.$ (Clearly $\bu_k = -\bK\bx$ is a fixed point of the
      iteration, and the $\frac{\sigma_{k-1} - \sigma_k}{\sigma_k}$ term is like the
      step-size of gradient descent.)</p>

      </subsubsection>
      <subsubsection><h1>Output-feedback</h1>

      <p>Returning to output feedback, we know that the optimal linear output-feedback
      policy from LQG is typically written in a state-space form, e.g.: \begin{gather*}
      \hat{\bx}[n+1] = \bA\hat{\bx}[n] + \bB_c\bu[n] + {\bf L}\left(\by[n] -
      \bC\hat{x}[n] - \bD\bu[n]\right), \\ \bu[n] = -\bK\hat{\bx}[n]. \end{gather*} But
      the Diffusion Policy architecture (with $H_u=1$) is typically formulated as
      learning a denoiser conditioned on a finite history of actions and observations,
      \begin{gather*}f_\theta(\bu[n], \sigma, \bar{\by}_{H_y}, \bar{\bu}_{H_y}), \\
      \bar{\by}_{H_y} = \left[\by[n-1],... ,\by[n-H_y]\right], \\ \bar{\bu}_{H_y} =
      \left[\bu[n-1],... ,\bu[n-H_y]\right].\end{gather*} So the more direct analogy
      would be to generating demonstrations from an autoregressive linear policy, e.g.
      $$\bu[n] = -\bK \begin{bmatrix} \bar{\by}_{H_y} \\ \bar{\bu}_{H_y}\end{bmatrix}.$$
      This autoregressive form is commonly used in <a
      href="output_feedback.html#disturbance-based">disturbance-based</a> output
      feedback, and may be helpful to think of it e.g. as an "unrolled" (truncated)
      Kalman filter. Again, we can achieve zero loss with a denoiser of the form $${\bf
      f}_\theta(\bu, \sigma, \bar{\by}_{H_y}, \bar{\bu}_{H_y}) =
      \frac{1}{\sigma}\left[\bu + \bK \begin{bmatrix} \bar{\by}_{H_y} \\
      \bar{\bu}_{H_y}\end{bmatrix}\right].$$</p>

      <todo>Notebook example</todo>

      <p>This shapes my current mental model for what Diffusion Policy is learning, even
      in complicated manipulation settings: it may be helpful to think about the history
      of actions and observations being compressed into a (task-relevant) belief-state
      representation, like we would have in an (unrolled, truncated) Kalman filter, that
      is sufficient for predicting actions.</p>

      </subsubsection>
      <subsubsection><h1>Action sequence prediction</h1>
      <p>Another important aspect of the Diffusion Policy architecture is that it is
      predicting not just the instantaneous action, but a sequence of future
      actions. Even in the linear policy setting, we can see that this puts different pressure on the learned representations...</p>

      <todo>
      <elib>Block23b</elib> ?
      </todo>

      </subsubsection>
    </subsection>
  
  </section>

  <section><h1>Inverse reinforcement learning</h1>

    <todo>Some good notes here: https://web.stanford.edu/class/cs237b/pdfs/lecture/lecture_10111213.pdf</todo>
  </section>

  <section><h1>Vistas</h1>
    <subsection><h1>Multitask / foundation models for control</h1>
    
      <p>If control directly from pixels was the first capability unlocked by imitation
      learning, I would say that large-scale multitask decision making is the second.
      Multitask learning has a long history in the machine learning community
      <elib>Zhang21</elib>, and multitask learning for control was popular first in
      "multitask RL" <todo>citation?</todo>. But the rise of foundation models
      <elib>Bommasani21</elib> in language and now in multimodal models, has completely
      changed our capabilities and our expectations. Suddenly, one can imagine a single
      policy, represented in a high-capacity neural network model, whose inputs are the
      robot sensors (including RGB cameras) plus a natural language command ("Robot,
      please make me a pizza"), and for the first time ever it seems like something like
      this could plausibly work.</p>

      <p>In my view, this has potentially profound implications for how we think about
      control. Our basic control definitions start with, e.g. we have a state $\bx$,
      inputs $\bu$, outputs $\by.$ The discussion on output feedback got us thinking a
      little about state representations for control -- for instance a belief state is a
      sufficient (but not necessary) state because it is a sufficient statistic of the
      history of actions and observations. But multitask in the imitation learning
      setting changes things. In the simple case, we'll say that our inputs $\bu$, and
      outputs $\by$ are the same across all of the tasks. But it may well be that the
      underlying state space is not. (I admit that philosophically there is a state of
      the entire universe which is the same across tasks, but I mean the more tractable
      representations of state that we've been using through the notes.) What does it
      mean to learn a state representation for control across tasks where even the
      dimension/cardinality of the state space can be different? Even our catch-all
      definition of belief state breaks down in this case.</p>

      <p>Are there tractable ways to describe distributions over tasks that are amenable
      to our strongest theoretical tools, but still relevant for the complexity and
      diversity of the real world? When we talked about stochastic optimal control, we
      gave examples where taking an average over many possible rollouts can actually
      simplify the loss landscape, avoiding some local minima and making optimization
      easier. Can multitask control formulations have a similar effect?</p>

      <p>Going further, how exactly is it that solving/learning one task can potentially
      help us in solving/learning another? This brings up basic questions about
      designing a curriculum for our control systems. Is is possible for us to soar to
      higher and higher heights if we sequence our control problem instances
      correctly?</p>

    </subsection>

    <subsection><h1>Distributed decentralized learning (aka "fleet learning")</h1>
    
      <p>When I start using phrases like "learning" and "curriculum", then it becomes
      very natural to think in terms of our natural intelligence. How did we learn to
      walk? To play tennis? But let's remember that these analogies only go so far. For
      me, the GPT series of models are clearly unlike any single natural intelligence,
      they are more like a collective intelligence of the entire species (though still
      certainly deficient in some metrics). In the age of foundation models, it may not
      be the case that every robot needs to learn to use a toaster; the dream of "fleet
      learning" is that one robot will learn how to use a toaster and then they will all
      have learned.</p>

      <p>This brings up fundamental questions about the learning algorithms, about data
      efficiency (and privacy). But it also challenges our theories of dynamics and
      control. For instance, there are open questions about how to balance being a
      generalist and using only shared data vs being a specialist. Certainly if a
      particular robot is solving problems in a particular warehouse, then while the
      statistics of tasks across the world may help form robust representations, this
      robot can almost certainly perform better if it narrows and specializes the
      policies (and world models) to exploit the distribution of tasks in the
      warehouse.</p>
      
      <p>A particular version of this question appears in the context of
      "cross-embodiment" data and models. Right now, robot data with action labels is
      scarce (compared with online data for text and images). This, in part, has
      motivated the use of datasets which combine data from many robots/platforms
      <elib>Shah23+Padalkar23</elib>, and architectures that can share representations
      and transfer learning even when the number/types of sensors and actuators changes
      across the fleet.</p>

    </subsection>

    <subsection><h1>Be rigorous</h1>

      <p>The fact that many of these fundamental questions are now being asked makes
      this a simply amazing time to be a roboticist. However, the pace of new
      innovations is so fast that often researchers feel pressure to race to publication
      before having done proper rigorous theoretical or empirical work. We are building
      tall towers but with somewhat shakey foundations. I firmly believe that the
      tenants of dynamics and control (amongst other rigorous technical tools) have a
      lot to contribute to understanding and continuing to push the field forward, and
      that some of the maturity with which we can understand these simpler (but not very
      simple!) problems can serve as a model for what we should expect about our
      understanding of the even more complex ones.</p>
  
    </subsection>
  </section>

</chapter>
<!-- EVERYTHING BELOW THIS LINE IS OVERWRITTEN BY THE INSTALL SCRIPT -->

<div id="references"><section><h1>References</h1>
<ol>

<li id=Argall09>
<span class="author">Brenna D Argall and Sonia Chernova and Manuela Veloso and Brett Browning</span>, 
<span class="title">"A survey of robot learning from demonstration"</span>, 
<span class="publisher">Robotics and autonomous systems</span>, vol. 57, no. 5, pp. 469--483, <span class="year">2009</span>.

</li><br>
<li id=Bagnell15>
<span class="author">J.A. Bagnell</span>, 
<span class="title">"An Invitation to Imitation"</span>, 
Tech. Report, CMU-RI-TR-15-08, March, <span class="year">2015</span>.

</li><br>
<li id=Osa18>
<span class="author">Takayuki Osa and Joni Pajarinen and Gerhard Neumann and J Andrew Bagnell and Pieter Abbeel and Jan Peters and others</span>, 
<span class="title">"An algorithmic perspective on imitation learning"</span>, 
<span class="publisher">Foundations and Trends in Robotics</span>, vol. 7, no. 1-2, pp. 1--179, <span class="year">2018</span>.

</li><br>
<li id=Pomerleau88>
<span class="author">Dean A Pomerleau</span>, 
<span class="title">"Alvinn: An autonomous land vehicle in a neural network"</span>, 
<span class="publisher">Advances in neural information processing systems</span>, vol. 1, <span class="year">1988</span>.

</li><br>
<li id=Bain95>
<span class="author">Michael Bain and Claude Sammut</span>, 
<span class="title">"A Framework for Behavioural Cloning."</span>, 
<span class="publisher">Machine Intelligence 15</span> , pp. 103--129, <span class="year">1995</span>.

</li><br>
<li id=Ziegler19>
<span class="author">Daniel M Ziegler and Nisan Stiennon and Jeffrey Wu and Tom B Brown and Alec Radford and Dario Amodei and Paul Christiano and Geoffrey Irving</span>, 
<span class="title">"Fine-tuning language models from human preferences"</span>, 
<span class="publisher">arXiv preprint arXiv:1909.08593</span>, <span class="year">2019</span>.

</li><br>
<li id=Zhang24>
<span class="author">Shengyu Zhang and Linfeng Dong and Xiaoya Li and Sen Zhang and Xiaofei Sun and Shuhe Wang and Jiwei Li and Runyi Hu and Tianwei Zhang and Fei Wu and Guoyin Wang</span>, 
<span class="title">"Instruction Tuning for Large Language Models: A Survey"</span>, 
, <span class="year">2024</span>.

</li><br>
<li id=Radford18>
<span class="author">Alec Radford and Karthik Narasimhan and Tim Salimans and Ilya Sutskever and others</span>, 
<span class="title">"Improving language understanding by generative pre-training"</span>, 
, <span class="year">2018</span>.

</li><br>
<li id=Radford19>
<span class="author">Alec Radford and Jeffrey Wu and Rewon Child and David Luan and Dario Amodei and Ilya Sutskever and others</span>, 
<span class="title">"Language models are unsupervised multitask learners"</span>, 
<span class="publisher">OpenAI blog</span>, vol. 1, no. 8, pp. 9, <span class="year">2019</span>.

</li><br>
<li id=Brown20>
<span class="author">Tom Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared D Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and others</span>, 
<span class="title">"Language models are few-shot learners"</span>, 
<span class="publisher">Advances in neural information processing systems</span>, vol. 33, pp. 1877--1901, <span class="year">2020</span>.

</li><br>
<li id=Bubeck23>
<span class="author">S&eacute;bastien Bubeck and Varun Chandrasekaran and Ronen Eldan and Johannes Gehrke and Eric Horvitz and Ece Kamar and Peter Lee and Yin Tat Lee and Yuanzhi Li and Scott Lundberg and others</span>, 
<span class="title">"Sparks of artificial general intelligence: Early experiments with gpt-4"</span>, 
<span class="publisher">arXiv preprint arXiv:2303.12712</span>, <span class="year">2023</span>.

</li><br>
<li id=Liu23>
<span class="author">Haotian Liu and Chunyuan Li and Qingyang Wu and Yong Jae Lee</span>, 
<span class="title">"Visual Instruction Tuning"</span>, 
, <span class="year">2023</span>.

</li><br>
<li id=Karamcheti24>
<span class="author">Siddharth Karamcheti and Suraj Nair and Ashwin Balakrishna and Percy Liang and Thomas Kollar and Dorsa Sadigh</span>, 
<span class="title">"Prismatic vlms: Investigating the design space of visually-conditioned language models"</span>, 
<span class="publisher">arXiv preprint arXiv:2402.07865</span>, <span class="year">2024</span>.

</li><br>
<li id=Laurençon24>
<span class="author">Hugo Laurençon and Léo Tronchon and Matthieu Cord and Victor Sanh</span>, 
<span class="title">"What matters when building vision-language models?"</span>, 
, <span class="year">2024</span>.

</li><br>
<li id=Brohan22>
<span class="author">Anthony	Brohan and Noah Brown and Justice Carbajal and Yevgen Chebotar and Joseph Dabis and Chelsea Finn and Keerthana Gopalakrishnan and Karol Hausman and Alex Herzog and Jasmine Hsu and Julian Ibarz and Brian Ichter and Alex Irpan and Tomas Jackson and Sally Jesmonth and Nikhil Joshi and Ryan Julian and Dmitry Kalashnikov and Yuheng Kuang and Isabel Leal and Kuang-Huei Lee and Sergey Levine and Yao Lu and Utsav Malla and Deeksha Manjunath and Igor Mordatch and Ofir Nachum and Carolina Parada and Jodilyn Peralta and Emily Perez and Karl Pertsch and Jornell Quiambao and Kanishka Rao and Michael Ryoo and Grecia Salazar and Pannag Sanketi and Kevin Sayed and Jaspiar Singh and Sumedh Sontakke and Austin Stone and Clayton Tan and Huong Tran and Vincent Vanhoucke and Steve Vega and Quan Vuong and Fei Xia and Ted Xiao and Peng Xu and Sichun Xu and Tianhe Yu and Brianna Zitkovich</span>, 
<span class="title">"RT-1: Robotics Transformer for Real-World Control at Scale"</span>, 
<span class="publisher">arXiv preprint arXiv:2212.06817</span> , <span class="year">2022</span>.

</li><br>
<li id=Brohan23>
<span class="author">Anthony Brohan and Noah Brown and Justice Carbajal and Yevgen Chebotar and Xi Chen and Krzysztof Choromanski and Tianli Ding and Danny Driess and Avinava Dubey and Chelsea Finn and Pete Florence and Chuyuan Fu and Montse Gonzalez Arenas and Keerthana Gopalakrishnan and Kehang Han and Karol Hausman and Alex Herzog and Jasmine Hsu and Brian Ichter and Alex Irpan and Nikhil Joshi and Ryan Julian and Dmitry Kalashnikov and Yuheng Kuang and Isabel Leal and Lisa Lee and Tsang-Wei Edward Lee and Sergey Levine and Yao Lu and Henryk Michalewski and Igor Mordatch and Karl Pertsch and Kanishka Rao and Krista Reymann and Michael Ryoo and Grecia Salazar and Pannag Sanketi and Pierre Sermanet and Jaspiar Singh and Anikait Singh and Radu Soricut and Huong Tran and Vincent Vanhoucke and Quan Vuong and Ayzaan Wahid and Stefan Welker and Paul Wohlhart and Jialin Wu and Fei Xia and Ted Xiao and Peng Xu and Sichun Xu and Tianhe Yu and Brianna Zitkovich</span>, 
<span class="title">"RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control"</span>, 
<span class="publisher">arXiv preprint arXiv:2307.15818</span> , <span class="year">2023</span>.

</li><br>
<li id=O’Neill24>
<span class="author">Abby O’Neill and Abdul Rehman and Abhiram Maddukuri and Abhishek Gupta and Abhishek Padalkar and Abraham Lee and Acorn Pooley and Agrim Gupta and Ajay Mandlekar and Ajinkya Jain and others</span>, 
<span class="title">"Open x-embodiment: Robotic learning datasets and rt-x models: Open x-embodiment collaboration 0"</span>, 
<span class="publisher">2024 IEEE International Conference on Robotics and Automation (ICRA)</span> , pp. 6892--6903, <span class="year">2024</span>.

</li><br>
<li id=Chi24>
<span class="author">Cheng Chi and Zhenjia Xu and Siyuan Feng and Eric Cousineau and Yilun Du and Benjamin Burchfiel and Russ Tedrake and Shuran Song</span>, 
<span class="title">"Diffusion Policy: Visuomotor Policy Learning via Action Diffusion"</span>, 
, <span class="year">2024</span>.

</li><br>
<li id=Zhao23>
<span class="author">Tony Z Zhao and Vikash Kumar and Sergey Levine and Chelsea Finn</span>, 
<span class="title">"Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware"</span>, 
<span class="publisher">arXiv preprint arXiv:2304.13705</span>, <span class="year">2023</span>.

</li><br>
<li id=Silver16>
<span class="author">David Silver and Aja Huang and Chris J Maddison and Arthur Guez and Laurent Sifre and George Van Den Driessche and Julian Schrittwieser and Ioannis Antonoglou and Veda Panneershelvam and Marc Lanctot and others</span>, 
<span class="title">"Mastering the game of Go with deep neural networks and tree search"</span>, 
<span class="publisher">nature</span>, vol. 529, no. 7587, pp. 484--489, <span class="year">2016</span>.

</li><br>
<li id=Levine16>
<span class="author">Sergey Levine and Chelsea Finn and Trevor Darrell and Pieter Abbeel</span>, 
<span class="title">"End-to-end training of deep visuomotor policies"</span>, 
<span class="publisher">The Journal of Machine Learning Research</span>, vol. 17, no. 1, pp. 1334--1373, <span class="year">2016</span>.

</li><br>
<li id=Florence20>
<span class="author">Peter Florence and Lucas Manuelli and Russ Tedrake</span>, 
<span class="title">"Self-Supervised Correspondence in Visuomotor Policy Learning"</span>, 
<span class="publisher">IEEE Robotics and Automation Letters</span>, vol. 5, no. 2, pp. 492-499, April, <span class="year">2020</span>.
[&nbsp;<a href="http://groups.csail.mit.edu/robotics-center/public_papers/Florence20.pdf">link</a>&nbsp;]

</li><br>
<li id=Chen20>
<span class="author">Dian Chen and Brady Zhou and Vladlen Koltun and Philipp Krahenbuhl</span>, 
<span class="title">"Learning by cheating"</span>, 
<span class="publisher">Conference on Robot Learning</span> , pp. 66--75, <span class="year">2020</span>.

</li><br>
<li id=Hwangbo19>
<span class="author">Jemin Hwangbo and Joonho Lee and Alexey Dosovitskiy and Dario Bellicoso and Vassilios Tsounis and Vladlen Koltun and Marco Hutter</span>, 
<span class="title">"Learning agile and dynamic motor skills for legged robots"</span>, 
<span class="publisher">Science Robotics</span>, vol. 4, no. 26, pp. eaau5872, <span class="year">2019</span>.

</li><br>
<li id=Chou23>
<span class="author">Glen Chou and Russ Tedrake</span>, 
<span class="title">"Synthesizing Stable Reduced-Order Visuomotor Policies for Nonlinear Systems via Sums-of-Squares Optimization"</span>, 
<span class="publisher">Proceedings of the IEEE 62nd Annual Conference on Decision and Control (CDC)</span> , <span class="year">2023</span>.
[&nbsp;<a href="http://groups.csail.mit.edu/robotics-center/public_papers/Chou23.pdf">link</a>&nbsp;]

</li><br>
<li id=Ross11>
<span class="author">St&eacute;phane Ross and Geoffrey Gordon and Drew Bagnell</span>, 
<span class="title">"A reduction of imitation learning and structured prediction to no-regret online learning"</span>, 
<span class="publisher">Proceedings of the fourteenth international conference on artificial intelligence and statistics</span> , pp. 627--635, <span class="year">2011</span>.

</li><br>
<li id=Chen21>
<span class="author">Lili Chen and Kevin Lu and Aravind Rajeswaran and Kimin Lee and Aditya Grover and Misha Laskin and Pieter Abbeel and Aravind Srinivas and Igor Mordatch</span>, 
<span class="title">"Decision transformer: Reinforcement learning via sequence modeling"</span>, 
<span class="publisher">Advances in neural information processing systems</span>, vol. 34, pp. 15084--15097, <span class="year">2021</span>.

</li><br>
<li id=Reed22>
<span class="author">Scott Reed and Konrad Zolna and Emilio Parisotto and Sergio Gomez Colmenarejo and Alexander Novikov and Gabriel Barth-Maron and Mai Gimenez and Yury Sulsky and Jackie Kay and Jost Tobias Springenberg and others</span>, 
<span class="title">"A generalist agent"</span>, 
<span class="publisher">arXiv preprint arXiv:2205.06175</span>, <span class="year">2022</span>.

</li><br>
<li id=Kim24>
<span class="author">Moo Jin Kim and Karl Pertsch and Siddharth Karamcheti and Ted Xiao and Ashwin Balakrishna and Suraj Nair and Rafael Rafailov and Ethan Foster and Grace Lam and Pannag Sanketi and Quan Vuong and Thomas Kollar and Benjamin Burchfiel and Russ Tedrake and Dorsa Sadigh and Sergey Levine and Percy Liang and Chelsea Finn</span>, 
<span class="title">"OpenVLA: An Open-Source Vision-Language-Action Model"</span>, 
<span class="publisher">arXiv preprint arXiv:2406.09246</span>, <span class="year">2024</span>.
[&nbsp;<a href="https://openvla.github.io/">link</a>&nbsp;]

</li><br>
<li id=Shafiullah22>
<span class="author">Nur Muhammad Shafiullah and Zichen Cui and Ariuntuya (Arty) Altanzaya and Lerrel Pinto</span>, 
<span class="title">"Behavior Transformers: Cloning k modes with one stone"</span>, 
<span class="publisher">Advances in Neural Information Processing Systems</span> , vol. 35, pp. 22955--22968, <span class="year">2022</span>.

</li><br>
<li id=Lee24>
<span class="author">Seungjae Lee and Yibin Wang and Haritheja Etukuru and H Jin Kim and Nur Muhammad Mahi Shafiullah and Lerrel Pinto</span>, 
<span class="title">"Behavior generation with latent actions"</span>, 
<span class="publisher">arXiv preprint arXiv:2403.03181</span>, <span class="year">2024</span>.

</li><br>
<li id=Florence22>
<span class="author">Pete Florence and Corey Lynch and Andy Zeng and Oscar A Ramirez and Ayzaan Wahid and Laura Downs and Adrian Wong and Johnny Lee and Igor Mordatch and Jonathan Tompson</span>, 
<span class="title">"Implicit behavioral cloning"</span>, 
<span class="publisher">Conference on Robot Learning</span> , pp. 158--168, <span class="year">2022</span>.

</li><br>
<li id=Bishop94>
<span class="author">Christopher M Bishop</span>, 
<span class="title">"Mixture density networks"</span>, 
, <span class="year">1994</span>.

</li><br>
<li id=Chi24a>
<span class="author">Cheng Chi and Zhenjia Xu and Chuer Pan and Eric Cousineau and Benjamin Burchfiel and Siyuan Feng and Russ Tedrake and Shuran Song</span>, 
<span class="title">"Universal Manipulation Interface: In-The-Wild Robot Teaching Without In-The-Wild Robots"</span>, 
, <span class="year">2024</span>.

</li><br>
<li id=Nakkiran24>
<span class="author">Preetum Nakkiran and Arwen Bradley and Hattie Zhou and Madhu Advani</span>, 
<span class="title">"Step-by-Step Diffusion: An Elementary Tutorial"</span>, 
, <span class="year">2024</span>.

</li><br>
<li id=Song20>
<span class="author">Jiaming Song and Chenlin Meng and Stefano Ermon</span>, 
<span class="title">"Denoising Diffusion Implicit Models"</span>, 
<span class="publisher">International Conference on Learning Representations</span> , <span class="year">2020</span>.

</li><br>
<li id=Permenter23>
<span class="author">Frank Permenter and Chenyang Yuan</span>, 
<span class="title">"Interpreting and Improving Diffusion Models Using the Euclidean Distance Function"</span>, 
<span class="publisher">arXiv preprint arXiv:2306.04848</span>, <span class="year">2023</span>.

</li><br>
<li id=Xu23>
<span class="author">Yilun Xu and Mingyang Deng and Xiang Cheng and Yonglong Tian and Ziming Liu and Tommi Jaakkola</span>, 
<span class="title">"Restart Sampling for Improving Generative Processes"</span>, 
<span class="publisher">Advances in Neural Information Processing Systems</span> , vol. 36, pp. 76806--76838, <span class="year">2023</span>.

</li><br>
<li id=Sohl-Dickstein15>
<span class="author">Jascha Sohl-Dickstein and Eric Weiss and Niru Maheswaranathan and Surya Ganguli</span>, 
<span class="title">"Deep unsupervised learning using nonequilibrium thermodynamics"</span>, 
<span class="publisher">International conference on machine learning</span> , pp. 2256--2265, <span class="year">2015</span>.

</li><br>
<li id=Hyvarinen05>
<span class="author">Aapo Hyvarinen</span>, 
<span class="title">"Estimation of Non-Normalized Statistical Models by Score Matching"</span>, 
<span class="publisher">Journal of Machine Learning Research</span>, vol. 6, pp. 695–708, <span class="year">2005</span>.

</li><br>
<li id=Vincent11>
<span class="author">Pascal Vincent</span>, 
<span class="title">"A connection between score matching and denoising autoencoders"</span>, 
<span class="publisher">Neural computation</span>, vol. 23, no. 7, pp. 1661--1674, <span class="year">2011</span>.

</li><br>
<li id=Song19>
<span class="author">Yang Song and Stefano Ermon</span>, 
<span class="title">"Generative Modeling by Estimating Gradients of the Data Distribution"</span>, 
<span class="publisher">Advances in Neural Information Processing Systems</span> , vol. 32, <span class="year">2019</span>.

</li><br>
<li id=Ho20>
<span class="author">Jonathan Ho and Ajay Jain and Pieter Abbeel</span>, 
<span class="title">"Denoising diffusion probabilistic models"</span>, 
<span class="publisher">Advances in neural information processing systems</span>, vol. 33, pp. 6840--6851, <span class="year">2020</span>.

</li><br>
<li id=Janner22>
<span class="author">Michael Janner and Yilun Du and Joshua Tenenbaum and Sergey Levine</span>, 
<span class="title">"Planning with Diffusion for Flexible Behavior Synthesis"</span>, 
<span class="publisher">International Conference on Machine Learning</span> , pp. 9902--9915, <span class="year">2022</span>.

</li><br>
<li id=Block23b>
<span class="author">Adam Block and Ali Jadbabaie and Daniel Pfrommer and Max Simchowitz and Russ Tedrake</span>, 
<span class="title">"Provable Guarantees for Generative Behavior Cloning: Bridging Low-Level Stability and High-Level Behavior"</span>, 
<span class="publisher">Thirty-seventh Conference on Neural Information Processing Systems</span> , <span class="year">2023</span>.

</li><br>
<li id=Zhang21>
<span class="author">Yu Zhang and Qiang Yang</span>, 
<span class="title">"A survey on multi-task learning"</span>, 
<span class="publisher">IEEE Transactions on Knowledge and Data Engineering</span>, vol. 34, no. 12, pp. 5586--5609, <span class="year">2021</span>.

</li><br>
<li id=Bommasani21>
<span class="author">Rishi Bommasani and Drew A Hudson and Ehsan Adeli and Russ Altman and Simran Arora and Sydney von Arx and Michael S Bernstein and Jeannette Bohg and Antoine Bosselut and Emma Brunskill and others</span>, 
<span class="title">"On the opportunities and risks of foundation models"</span>, 
<span class="publisher">arXiv preprint arXiv:2108.07258</span>, <span class="year">2021</span>.

</li><br>
<li id=Shah23>
<span class="author">Dhruv Shah and Ajay Sridhar and Nitish Dashora and Kyle Stachowicz and Kevin Black and Noriaki Hirose and Sergey Levine</span>, 
<span class="title">"ViNT: A Foundation Model for Visual Navigation"</span>, 
<span class="publisher">Conference on Robot Learning</span> , pp. 711--733, <span class="year">2023</span>.

</li><br>
<li id=Padalkar23>
<span class="author">Abhishek Padalkar and Acorn Pooley and Ajinkya Jain and Alex Bewley and Alex Herzog and Alex Irpan and Alexander Khazatsky and Anant Rai and Anikait Singh and Anthony Brohan and others</span>, 
<span class="title">"Open x-embodiment: Robotic learning datasets and rt-x models"</span>, 
<span class="publisher">arXiv preprint arXiv:2310.08864</span>, <span class="year">2023</span>.

</li><br>
</ol>
</section><p/>
</div>

<table style="width:100%;"><tr style="width:100%">
  <td style="width:33%;text-align:left;"><a class="previous_chapter" href=rl_policy_search.html>Previous Chapter</a></td>
  <td style="width:33%;text-align:center;"><a href=index.html>Table of contents</a></td>
  <td style="width:33%;text-align:right;"><a class="next_chapter" href=drake.html>Next Chapter</a></td>
</tr></table>

<div id="footer">
  <hr>
  <table style="width:100%;">
    <tr><td><a href="https://accessibility.mit.edu/">Accessibility</a></td><td style="text-align:right">&copy; Russ
      Tedrake, 2024</td></tr>
  </table>
</div>


</body>
</html>
