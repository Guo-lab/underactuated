<!DOCTYPE html>

<html>

  <head>
    <title>Ch. 15 - 
Output Feedback (aka Pixels-to-Torques)</title>
    <meta name="Ch. 15 - 
Output Feedback (aka Pixels-to-Torques)" content="text/html; charset=utf-8;" />
    <link rel="canonical" href="http://underactuated.mit.edu/output_feedback.html" />

    <script src="https://hypothes.is/embed.js" async></script>
    <script type="text/javascript" src="chapters.js"></script>
    <script type="text/javascript" src="htmlbook/book.js"></script>

    <script src="htmlbook/mathjax-config.js" defer></script>
    <script type="text/javascript" id="MathJax-script" defer
      src="htmlbook/MathJax/es5/tex-chtml.js">
    </script>
    <script>window.MathJax || document.write('<script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" defer><\/script>')</script>

    <link rel="stylesheet" href="htmlbook/highlight/styles/default.css">
    <script src="htmlbook/highlight/highlight.pack.js"></script> <!-- http://highlightjs.readthedocs.io/en/latest/css-classes-reference.html#language-names-and-aliases -->
    <script>hljs.initHighlightingOnLoad();</script>

    <link rel="stylesheet" type="text/css" href="htmlbook/book.css" />
  </head>

<body onload="loadChapter('underactuated');">

<div data-type="titlepage">
  <header>
    <h1><a href="index.html" style="text-decoration:none;">Underactuated Robotics</a></h1>
    <p data-type="subtitle">Algorithms for Walking, Running, Swimming, Flying, and Manipulation</p>
    <p style="font-size: 18px;"><a href="http://people.csail.mit.edu/russt/">Russ Tedrake</a></p>
    <p style="font-size: 14px; text-align: right;">
      &copy; Russ Tedrake, 2024<br/>
      Last modified <span id="last_modified"></span>.</br>
      <script>
      var d = new Date(document.lastModified);
      document.getElementById("last_modified").innerHTML = d.getFullYear() + "-" + (d.getMonth()+1) + "-" + d.getDate();</script>
      <a href="misc.html">How to cite these notes, use annotations, and give feedback.</a><br/>
    </p>
  </header>
</div>

<p><b>Note:</b> These are working notes used for <a
href="https://underactuated.csail.mit.edu/Spring2024/">a course being taught
at MIT</a>. They will be updated throughout the Spring 2024 semester.  <a
href="https://www.youtube.com/playlist?list=PLkx8KyIQkMfU5szP43GlE_S1QGSPQfL9s">Lecture videos are available on YouTube</a>.</p>

<table style="width:100%;"><tr style="width:100%">
  <td style="width:33%;text-align:left;"><a class="previous_chapter" href=robust.html>Previous Chapter</a></td>
  <td style="width:33%;text-align:center;"><a href=index.html>Table of contents</a></td>
  <td style="width:33%;text-align:right;"><a class="next_chapter" href=limit_cycles.html>Next Chapter</a></td>
</tr></table>

<script type="text/javascript">document.write(notebook_header('output_feedback'))
</script>
<!-- EVERYTHING ABOVE THIS LINE IS OVERWRITTEN BY THE INSTALL SCRIPT -->
<chapter style="counter-reset: chapter 14"><h1>
Output Feedback (aka Pixels-to-Torques)</h1>

  <p>In this chapter we will finally start considering systems of the form:
  \begin{gather*} \bx[n+1] = {\bf f}(\bx[n], \bu[n], \bw[n]) \\ \by[n] =
  {\bf g}(\bx[n], \bu[n], \bv[n]),\end{gather*} where most of these symbols
  have been <a href="robust.html">described before</a>, but we have now added
  $\by[n]$ as the output of the system, and $\bv[n]$ which is representing
  "measurement noise" and is typically the output of a random process.  In
  other words, we'll finally start addressing the fact that we have to make
  decisions based on sensor measurements -- most of our discussions until now
  have tacitly assumed that we have access to the true state of the system for
  use in our feedback controllers (and that's already been a hard problem).
  </p>

  <div id="plant_diagram">
    <script src="htmlbook/js-yaml.min.js"></script>
    <script type="text/javascript">
    var sys = jsyaml.load(`
name: <b>Plant</b>
input_ports:
- $\\bu$
- $\\bw$
- $\\bv$
output_ports:
- $\\by$`);
    document.write(system_html(sys));
    </script>
  </div> 

  <p>In some cases, we will see that the assumption of "full-state feedback" is
  not so bad -- we do have good tools for state estimation from raw sensor data.
  But even our best state estimation algorithms do add some dynamics to the
  system in order to filter out noisy measurements; if the time constants of
  these filters is near the time constant of our dynamics, then it becomes
  important that we include the dynamics of the estimator in our analysis of the
  closed-loop system.</p>

  <p>In other cases, it's entirely too optimistic to design a controller
  assuming that we will have an estimate of the full state of the system.  Some
  state variables might be completely unobservable, others might require
  specific "information-gathering" actions on the part of the controller.</p>

  <p>For me, the problem of robot manipulation is one important application
  domain where more flexible approaches to output feedback become critically
  important. Imagine you are trying to design a controller for a robot that
  needs to button the buttons on your shirt.  Our current tools would require
  us to first estimate the state of the shirt (how many degrees of freedom does
  my shirt have?); but certainly the full state of my shirt should not be
  required to button a single button.  Or if you want to program a robot to
  make a salad -- what's the state of the salad?  Do I really need to know the
  positions and velocities of every piece of lettuce in order to be successful?
  These questions are (finally) getting a lot of attention in the research
  community these days, under the umbrella of "learning state representations".
  But what does it mean to be a good state representation?  There are a number
  of simple lessons from output feedback in control that can shed light on this
  fundamental question.
  </p>

  <section><h1>Background</h1>
    <subsection><h1>The classical perspective</h1>

      <p>To some extent, this idea of calling out "output feedback" as an
      advanced topic is a relatively new problem.  Before state-space and
      optimization-based approaches to control ushered in "modern control", we
      had "classical control".  Classical control focused predominantly (though
      not exclusively) on linear time-invariant (LTI) systems, and made very
      heavy use of frequency-domain analysis (e.g. via the Fourier
      Transform/Laplace Transform).  There are many excellent books on the
      subject; <elib>Hespanha09+Astrom10</elib> are nice examples of modern
      treatments that start with state-space representations but also treat the
      frequency-domain perspective. "Pole placement" and "loop shaping" are some
      of the tools of this trade.</p>

      <p>What's important for us to acknowledge here is that in classical
      control, basically everything was built around the idea of output
      feedback.  The fundamental concept is the transfer function of a system,
      which is a input-to-output map (in frequency domain) that can completely
      characterize an LTI system.  Core concepts like pole placement and loop
      shaping were fundamentally addressing the challenge of output feedback
      that we are discussing here.  Sometimes I feel that, despite all of the
      things we've gained with modern, optimization-based control, I worry that
      we've lost something in terms of considering rich characterizations of
      closed-loop performance (rise time, dwell time, overshoot, ...) and
      perhaps even in practical robustness of our systems to unmodeled
      errors.</p>

      <todo>Add a few examples here that capture it.</todo>

    </subsection>

    <subsection><h1>From pixels to torques</h1>

      <p>Just like some of our oldest approaches to control were fundamentally solving
      an output feedback problem, some of our newest approaches to control are doing it,
      too. Deep learning has revolutionized computer vision, and "deep imitation
      learning" and "deep reinforcement learning" have been a recent source of many
      impressive demonstrations of control systems that can operate directly from pixels
      (e.g. consuming the output of a deep perception system), without explicitly
      representing nor estimating the full state of the system.</p>
      
      <p>Imitation learning works incredibly well these days, even for very complicated
      tasks (e.g. <elib>Chi24+Zhao23</elib>). One could argue that imitation learning is
      not solving the full control problem -- once somebody has solved a control
      problem, the imitation learning system is able to use supervised learning to map
      the pixels to torques in a high-capacity neural network model. We'll cover these
      techniques in some detail in a future chapter. In order to generalize well to
      states not present in the training data, or to new tasks in a
      (language-conditioned) multitask framework, the imitation learner must be
      acquiring some implicit state representation of the world. Perhaps we can leverage
      these learned state representations to synthesize additional controllers for new
      tasks.</p>
      
      <p>Reinforcement learning (RL) definitely is solving difficult control problems,
      though it currently still requires quite a bit of guidance (via cost function /
      environment tuning) and requires an exhorbitant amount of computation.
      Interestingly, it's quite common in reinforcement learning to solve a full-state
      feedback problem first, and then to use "teacher student" (once again a form of
      imitation learning) to distill the full-state feedback controller into an output
      feedback control (e.g. <elib>Miki22+Chen23</elib>). Normally this is motivated by
      training efficiency -- this way one does not need to include the renderer in the
      simulation early in learning -- but it's not completely clear to me how much this
      is helping the optimizer as well. There is no doubt that we would like to have
      stronger RL algorithms that succeed more often and/or require less up-front
      simulation engineering and compute.</p>

      <p>The synthesis of ideas between machine learning (both theoretical and applied)
      and control theory is one of the most exciting and productive frontiers for
      research today.  I am highly optimistic that we will be able to uncover the
      underlying principles and help transition this budding field into a technology. My
      goal for this chapter is to summarize some of the key lessons from control that
      can, I hope, help with that discussion.</p>
    </subsection>

  </section>

  <section id="static"><h1>Static Output Feedback</h1>
  
    <p>One of the extremely important almost unstated lessons from dynamic
    programming with additive costs and the Bellman equation is that the
    optimal policy can <i>always</i> be represented as a function $\bu^*
    = \pi^*(\bx).$ So far in these notes, we've assumed that the controller has
    direct access to the true state, $\bx$. In this chapter, we are finally
    removing that assumption. Now the controller only has direct access to the
    potentially noisy observations $\by$.</p>

    <p>So the natural first question to ask might be, what happens if we write
    our policies now as a function, $\bu = \pi(\by)?$  This is known as
    "static" output feedback, in contrast to "dynamic" output feedback where
    the controller is not a static function, but is itself another input-output
    dynamical system. Unfortunately, in the general case it is <i>not</i> the
    case that optimal policies can be perfectly represented with static output
    feedback.  But one can still try to solve an optimal control problem where
    we restrict our search to static policies; our goal will be to find the
    best controller in this class to minimize the cost.</p>

    <div id="static_output_feedback_diagram">
      <script type="text/javascript">
      var sys = jsyaml.load(`
  name: <b>Policy</b> (static output feedback)
  input_ports:
  - $\\by$
  output_ports:
  - $\\bu$`);
      document.write(system_html(sys));
      </script>
    </div>

    <subsection><h1>A hardness result</h1>

      <p>Let's consider the problem of designing a static output feedback for linear
      systems (surely this should be the easy case!): $\dot\bx = \bA\bx + \bB\bu, \by =
      \bC\bx.$ Unfortunately, <elib>Blondel97</elib> showed that the question of whether
      a stabilizing static output feedback $\bu = -\bK \by$ even exists for a given
      system of the form $$\dot\bx = \bA\bx + \bB\bu,\quad \by = \bC \bx,$$ is NP-hard.
      The set of stabilizing ${\bf K}$ matrices may be not only nonconvex, but actually
      disconnected.  We can see that with a simple example (given to me once during a
      conversation with Alex Megretski). 
      </p>

    <example id="static_output_feedback">
      <h1>Parameterizations of Static Output Feedback</h1>

      <p>Consider the single-input, single-output LTI system $$\dot{\bx} = {\bf
      A}\bx + {\bf B} u, \quad y = {\bf C}\bx,$$ with $${\bf A} = \begin{bmatrix}
      0 & 0 & 2 \\ 1 & 0 & 0 \\ 0 & 1 & 0\end{bmatrix}, \quad {\bf B} =
      \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}, \quad {\bf C} = \begin{bmatrix}
      1 & 1 & 3 \end{bmatrix}.$$  Here the linear static-output-feedback policy
      can be written as $u = -ky$, with a single scalar parameter $k$. </p>

      <p>Here is a plot of the maximum eigenvalue (real-part) of the closed-loop
      system, as a function of $k.$  The system is only stable when this maximum
      value is less than zero.  You'll find the set of stabilizing $k$'s is a
      disconnected set.</p>

      <figure><img width="80%" src="data/static_feedback_disconnected.svg"/></figure>

      </example>

      <p>Just because this problem is NP-hard doesn't mean we can't find good
      controllers in practice. Some of the recent results from reinforcement
      learning have reminded us of this. We should not expect an efficient
      globally optimal algorithm that works for every problem instance; but we
      should absolutely keep working on the problem. Perhaps the class of
      problems that our robots will actually encounter in the real world is a
      easier than this general case (the standard examples of bad cases in
      linear systems, e.g. with interleaved poles and zeros, do feel a bit
      contrived and unlikely to occur in practice).</p>

    </subsection>

    <todo>Jack had another nice example in his poly search lecture:
    https://youtu.be/JhjROrZxBhM?t=1099 and "what goes wrong in output
    feedback?" https://youtu.be/JhjROrZxBhM?t=5066</todo>

    <todo>Bilinear alternations with SOS</todo>
  
    <subsection><h1>Perhaps a history of observations?</h1>
    
      <p>In addition to giving a potentially bad optimization landscape, it's pretty
      easy to convince yourself that making decisions only based on the current
      observations is not sufficient for decision making in all problems. Take, for
      example, the LQR controller we derived for balancing an Acrobot or Cart Pole --
      these controllers were a function of the full state (positions and velocities). If
      our robot is only instrumented with position sensors (e.g. encoders), or if our
      observations are instead coming from a camera pointed at the robot, then by
      looking at only the instantaneous observations would not have any information
      about the joint velocities. Although it would have to be shown, it seems
      reasonable to expect that no controller of this form could accomplish the
      balancing task (even for the linearized system) from all initial conditions.</p>
    
      <p>One common response today is: what if we make the controller a function of some
      finite recent <i>history</i> of observations? If there is no measurement noise,
      then the last two position measurements are sufficient to make a (slightly
      delayed) estimate of the velocity. If there is measurement noise, then taking a
      slightly longer history would allow us to filter out some of that noise. Yes! This
      is on the right track. Technically this would no longer be a "static" controller;
      it requires memory to store the previous observations and memory is the defining
      characteristic of a "dynamic" controller. But how much history/memory do we
      need?</p>

    </subsection>

  </section>
  
  <section id="pomdp"><h1>Partially-observable Markov Decision Processes (POMDPs)</h1>

    <p>For an upper bound on the answer to the question "how much memory do we need?",
    we can turn to the key results from the framework of POMDPS. The short answer is
    that if we use the <i>entire</i> history of actions and observations, then this is
    clearly sufficient for optimal decision making. Rather than maintaining this
    expanding history explicitly, we can instead maintain a representation that is a
    <i>sufficient statistic</i> for this history. In the good cases, we can find
    sufficient statistics that have a finite representation.</p>

    <p>A (finite) partially-observable Markov decision process (POMDP) is a stochastic
    state-space dynamical system with discrete (but potentially infinite) states $S$,
    actions $A$, observations $O$, initial condition probabilities $p_{s_0}(s)$,
    transition probabilities $p(s'|s,a)$, observation probabilities $p_{o|s}(o | s)$, and
    a deterministic cost function $\ell: S \times A \rightarrow \Re$
    <elib>Kaelbling98</elib>.  Our goal is to minimize the finite-horizon expected
    cost, $E[\sum_{n=1}^N \ell(s[n], u[n])].$ </p>

    <p>An important idea for POMDP's is the notion of the <i>belief state</i> of a
    POMDP: \[ b_i[n] = P(s[n] = s_i | a[0] = a_0, o[0] = o_0, ... a_{n-1}, o_{n-1}).\]
    ${\bf b}[n] \in \mathcal{B}[n] \subset \Delta^{|S|}$ where $\Delta^m \subset \Re^m$ is the
    $m$-dimensional simplex and we use $b_i[n]$ to denote the $i$th element.  ${\bf
    b}[n]$ is a sufficient statistic for the POMDP, in the sense that it captures all
    information about the history that can be used to predict the future state (and
    therefore also observations, and rewards) of the process.  Using ${\bf h}_n = [a_0, o_0,
    ..., a_{n-1}, o_{n-1}]$ to denote the history of all actions and observations, we
    have  $$P(s[n+1] = s | {\bf b}[n] = {\bf b}_n, {\bf h}[n] = {\bf h}_n, a[n] = a_n) = P(s[n+1] = s | {\bf b}[n] = {\bf b}_n, a[n] = a_n).$$  It is
    also <i>sufficient for optimal control</i>, in the sense that the optimal policy can
    be written as $a^*[n] = \pi^*({\bf b}[n], n).$</p>

    <p>The belief state is <i>observable</i>, with the optimal Bayesian
    estimator/observer given by the (belief-)state-space form as $$
    b_i[n+1] = f_i({\bf b}[n], a[n], o[n]),$$  where $b_i[0] = p_{s_0}(s_i)$, and $$f_i({\bf b}, a, o) = \frac{p_{o|s}(o \,|\, s_i) \sum_{j \in
      |S|} p(s_i\,|\,s_j,a) b_j}{p_{o|b,a}(o \,|\, {\bf b},
      a)} = \frac{p_{o|s}(o \,|\, s_i) \sum_{j \in
        |S|} p(s_i\,|\,s_j,a) b_j}{\sum_{i \in |S|} p_{o|s}(o \,|\, s_i) \sum_{j \in |S|}
        p(s_i | s_j, a) b_j}.$$  Using the <a
    href="http://128.30.27.120/underactuated/stochastic.html#mdp">transition matrix
    notation from MDPs</a> we can rewrite this in vector form as $$f({\bf b}, a, o) =
    \frac{\text{diag}({\bf c}(o)) \, {\bf T}(a) \, {\bf b}}{{\bf c}^T(o) \, {\bf T}(a)
    \, {\bf b}},$$ where ${\bf c}(o)$ is the column vector with $c_i(o) = p_{o|s}(o
    \,|\, s_i).$ Moreover, we can use this belief update to form the ``belief MDP'' for
    planning by marginalizing over the future observations, giving the \begin{align*}
    p_{\text{bMDP}}({\bf b}'|{\bf b}, a) = \sum_{i \in |O|} p({\bf b}'| {\bf b}, a, o_i)
    p_{o |b,a}(o_i | {\bf b}, a) = \sum_{i \in |O|} p({\bf b}'| {\bf b}, a, o_i) {\bf
    c}^T(o_i) \, {\bf T}(a) \, {\bf b},\end{align*} where $p({\bf b}'| {\bf b}, a, o)$
    is 1 if ${\bf b}' = f({\bf b},a,o)$ and 0 otherwise.</p>
    
    <p>The belief MDP is a little scary -- we are evolving distributions over possible
    beliefs! Perhaps one small consolation is that for discrete problems, even though
    ${\bf b}[n]$ is represented with a vector of real values, it actually lives in a
    finite space, $\mathcal{B}[n]$, because only a finite set of beliefs are reachable
    in $n$ steps given finite actions and observations. The policy $\pi^*({\bf b}[n],
    n)$ which optimal state feedback for the belief MDP is exactly the optimal output
    feedback for the original POMDP.</p> 

    <example><h1>The Cheeze Maze</h1>

      <p>One of the classic simple examples from the world of POMDPs is the "cheese
      maze"<elib>McCallum92</elib> (though it's not much of a maze). The robot mouse can
      move one square at a time, the observations are the integer labels drawn on the
      map below. The goal is the shaded region.</p>

      <figure><img width="50%" src="figures/cheesemaze.png"/></figure>

      <p>Imagine you are placed at a (uniform) random position in the maze. You do get
      to know the map, but you don't get to know your location. What would be your
      strategy for navigating to the goal? If your initial observation is 5, you have a
      decision to make... are you optimistic and try moving down (in the hopes that
      you'll find the goal?), or do you move up to see a 1, 3, or 4? What about if your
      initial observation is a 2?</p>
    
      <p>One can write the optimal policy for this specific problem with exactly 7
      discrete (belief) states.  Can you find them?</p>

    </example>

    <todo>
    <p>It's not immediately clear how to losslessly represent distributions over
    possible beliefs (functions over $\Re^{|S|}$ in the finite state case), but it turns
    out that the value function has some known important properties...</p></todo>

    <p>More generally, in continuous state spaces, the belief state is a function ${\bf
    b}(\bx, n) = p_{x|h}(\bx[n] \,|\, {\bf h}[n]):$</p>

    <div id="belief_policy">
      <center>
      <table>
        <tr><td>
      <script type="text/javascript">
      var sys = jsyaml.load(`
  name: <b>Bayes Filter</b>
  input_ports:
  - $\\by$ 
  - $\\bu$
  output_ports:
  - $\{\\bf b\}(\\bx)$`);
      document.write(system_html(sys));
      </script>
      </td><td>
      <script type="text/javascript">
        var sys = jsyaml.load(`
    name: <b>Belief Policy</b>
    input_ports:
    - ""
    output_ports:
    - $\\bu$.`);
        document.write(system_html(sys));
        </script>
      </td></tr>
      </table>
    </center>
    </div>

    <p>There is a wealth of literature on POMDPs in robotics, and on POMDP solution techniques. See <elib>Lauri22</elib> for a nice recent survey.</p>

  </section>

  <section><h1>Linear systems w/ Gaussian noise</h1>
  
    <subsection><h1 id="lqg">Linear Quadratic Regulator w/ Gaussian Noise
      (LQG)</h1>
    </subsection>
  
    <subsection><h1>Trajectory optimization with Iterative LQG</h1></subsection>
    <todo>Note: I also have a <a href="belief.html">draft chapter on planning
    under uncertainty</a></todo>
  
  </section>

  <section><h1>Observer-based Feedback</h1>

    <p>Since we know so much about designing full-state feedback controllers,
    one of the most natural (and dominant) approaches to control is to first <a
    href="state_estimation.html">design an observer</a> (aka "state
    estimator"), and then to use state feedback. Famously, this approach is
    actually optimal for the quadratic regular objective on linear-Gaussian
    systems (LQG) -- this is known as the "separation principle". But it is
    certainly not optimal in general!</p>

    <subsection><h1>Luenberger Observer</h1></subsection>


  </section>

  <section><h1 id="disturbance-based">Disturbance-based feedback</h1>
  
    <p>There is an interesting alternative to trying to observe/estimate the true state
    of the system, which can in some cases lead to convex formulations of the
    output-feedback objective. The notion of disturbance-based feedback, which we <a
    href="robust.html#disturbance-based">introduced in the context of stochastic/robust
    MPC</a>, can be extended to disturbance-based <i>output</i> feedback:
    \begin{gather*} \bx[n+1] = \bA\bx[n] + \bB\bu[n] + \bw[n],\\ \by[n] = \bC\bx[n] +
    \bv[n], \end{gather*} by parameterizing an output feedback policy of the form
    $$\bu[n] = \bK_0[n] \by[0] + \sum_{i=1}^{n-1}\bK_i[n]{\bf e}[n-i],$$ where ${\bf
    e}[n] = ...$ <elib>Sadraddini20</elib>.</p>

    <!-- <subsection><h1>SLS</h1>
    
    </subsection> -->
  
  </section>

  <section><h1>Optimizing dynamic policies</h1>
  
    <todo>State-space models.  ARX Models.</todo>

    <subsection>
      <h1>Convex reparameterizations of $H_2$, $H_\infty$, and LQG</h1>

      <p>DGKF (solving two Riccati equations)<elib>Doyle88</elib></p>

      <p>Scherer's convex reparameterizations of LQG<elib
      part="&sect;4.2">Scherer15</elib>.
      </p>

    </subsection>

    <subsection><h1>Policy gradient for LQG</h1>
    
    </subsection>

    <subsection><h1>Sums-of-squares alternations</h1>

      <p>Coming soon.  See, for instance, <elib>Chou23</elib>.</p>
    
    </subsection>

    <subsection><h1>Teacher-student learning</h1>

      <todo>as seen in Marco Hutter, Pulkit, ...</todo>

    </subsection>
    
        
  </section>

  <todo>Task-relevant variables / learning state representations (reference "task-relevant models" section of the sysid notes.</todo>

  <section><h1>Feedback from pixels</h1>
  
    <p>In my opinion, one of the most important advances in control in the last
    decade has been the introduction of high-rate feedback from cameras. This
    advance was enabled by the revolution in computer vision that came with
    deep learning. Especially in the domain of robotic manipulation, the value
    of this feedback is undeniable. Unfortunately, these sensors break many of
    the synthesis tools that we've discussed in the notes -- not only are they
    very high dimensional, but the space of RGB images is horrible and
    non-smooth. As of this writing, conventional wisdom is that model-based
    control does not have a lot to offer to this problem -- to design control
    from cameras, we are often limited to either imitation learning or
    black-box reinforcement learning. (I personally think that we have <a
    href="https://en.wikipedia.org/wiki/Don%27t_throw_the_baby_out_with_the_bathwater">thrown
    the baby out with the bathwater</a>, and consider a highly important
    research area to close this gap.)</p>

    <p>More coming soon...</p>

    <todo>Visuomotor policies that avoid explicit RGB. E.g. Glen's work w/ keypoints +
    SOS.</todo>

  </section>

</chapter>
<!-- EVERYTHING BELOW THIS LINE IS OVERWRITTEN BY THE INSTALL SCRIPT -->

<div id="references"><section><h1>References</h1>
<ol>

<li id=Hespanha09>
<span class="author">Joao P. Hespanha</span>, 
<span class="title">"Linear Systems Theory"</span>, Princeton Press
, <span class="year">2009</span>.

</li><br>
<li id=Astrom10>
<span class="author">Karl Johan {\AA}str{\"o}m and Richard M Murray</span>, 
<span class="title">"Feedback systems: an introduction for scientists and engineers"</span>, Princeton university press
, <span class="year">2010</span>.

</li><br>
<li id=Chi24>
<span class="author">Cheng Chi and Zhenjia Xu and Siyuan Feng and Eric Cousineau and Yilun Du and Benjamin Burchfiel and Russ Tedrake and Shuran Song</span>, 
<span class="title">"Diffusion Policy: Visuomotor Policy Learning via Action Diffusion"</span>, 
, <span class="year">2024</span>.

</li><br>
<li id=Zhao23>
<span class="author">Tony Z Zhao and Vikash Kumar and Sergey Levine and Chelsea Finn</span>, 
<span class="title">"Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware"</span>, 
<span class="publisher">arXiv preprint arXiv:2304.13705</span>, <span class="year">2023</span>.

</li><br>
<li id=Miki22>
<span class="author">Takahiro Miki and Joonho Lee and Jemin Hwangbo and Lorenz Wellhausen and Vladlen Koltun and Marco Hutter</span>, 
<span class="title">"Learning robust perceptive locomotion for quadrupedal robots in the wild"</span>, 
<span class="publisher">Science Robotics</span>, vol. 7, no. 62, pp. eabk2822, <span class="year">2022</span>.

</li><br>
<li id=Chen23>
<span class="author">Tao Chen and Megha Tippur and Siyang Wu and Vikash Kumar and Edward Adelson and Pulkit Agrawal</span>, 
<span class="title">"Visual dexterity: In-hand reorientation of novel and complex object shapes"</span>, 
<span class="publisher">Science Robotics</span>, vol. 8, no. 84, <span class="year">2023</span>.

</li><br>
<li id=Blondel97>
<span class="author">Vincent Blondel and John N Tsitsiklis</span>, 
<span class="title">"{NP}-hardness of some linear control design problems"</span>, 
<span class="publisher">SIAM journal on control and optimization</span>, vol. 35, no. 6, pp. 2118--2127, <span class="year">1997</span>.

</li><br>
<li id=Kaelbling98>
<span class="author">Leslie Pack Kaelbling and Michael L. Littman and Anthony R. Cassandra</span>, 
<span class="title">"Planning and Acting in Partially Observable Stochastic Domains"</span>, 
<span class="publisher">Artificial Intelligence</span>, vol. 101, January, <span class="year">1998</span>.

</li><br>
<li id=McCallum92>
<span class="author">R. Andrew McCallum</span>, 
<span class="title">"First results with utile distinction memory for reinforcement learning"</span>, 
, <span class="year">1992</span>.

</li><br>
<li id=Lauri22>
<span class="author">Mikko Lauri and David Hsu and Joni Pajarinen</span>, 
<span class="title">"Partially observable markov decision processes in robotics: A survey"</span>, 
<span class="publisher">IEEE Transactions on Robotics</span>, vol. 39, no. 1, pp. 21--40, <span class="year">2022</span>.

</li><br>
<li id=Sadraddini20>
<span class="author">Sadra Sadraddini and Russ Tedrake</span>, 
<span class="title">"Robust Output Feedback Control with Guaranteed Constraint Satisfaction"</span>, 
<span class="publisher">In the Proceedings of 23rd ACM International Conference on Hybrid Systems: Computation and Control</span> , pp. 12, April, <span class="year">2020</span>.
[&nbsp;<a href="http://groups.csail.mit.edu/robotics-center/public_papers/Sadraddini20.pdf">link</a>&nbsp;]

</li><br>
<li id=Doyle88>
<span class="author">John Doyle and Keith Glover and Pramod Khargonekar and Bruce Francis</span>, 
<span class="title">"State-space solutions to standard H 2 and H&#8734; control problems"</span>, 
<span class="publisher">1988 American Control Conference</span> , pp. 1691--1696, <span class="year">1988</span>.

</li><br>
<li id=Scherer15>
<span class="author">Carsten Scherer and Siep Weiland</span>, 
<span class="title">"Linear {Matrix} {Inequalities} in {Control}"</span>, Online Draft
, pp. 293, <span class="year">2015</span>.

</li><br>
<li id=Chou23>
<span class="author">Glen Chou and Russ Tedrake</span>, 
<span class="title">"Synthesizing Stable Reduced-Order Visuomotor Policies for Nonlinear Systems via Sums-of-Squares Optimization"</span>, 
<span class="publisher">Under review</span> , <span class="year">2023</span>.
[&nbsp;<a href="http://groups.csail.mit.edu/robotics-center/public_papers/Chou23.pdf">link</a>&nbsp;]

</li><br>
</ol>
</section><p/>
</div>

<table style="width:100%;"><tr style="width:100%">
  <td style="width:33%;text-align:left;"><a class="previous_chapter" href=robust.html>Previous Chapter</a></td>
  <td style="width:33%;text-align:center;"><a href=index.html>Table of contents</a></td>
  <td style="width:33%;text-align:right;"><a class="next_chapter" href=limit_cycles.html>Next Chapter</a></td>
</tr></table>

<div id="footer">
  <hr>
  <table style="width:100%;">
    <tr><td><a href="https://accessibility.mit.edu/">Accessibility</a></td><td style="text-align:right">&copy; Russ
      Tedrake, 2024</td></tr>
  </table>
</div>


</body>
</html>
